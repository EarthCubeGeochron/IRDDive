```{r load_data, message=FALSE, warning = FALSE}

devtools::install_github('EarthCubeGeochron/geodiveR')

library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
library(data.table)

sourcing <- list.files('R', full.names = TRUE) %>% 
  map(source, echo = FALSE, print = FALSE, verbose = FALSE)

publications <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp <- readr::read_tsv('input/sentences_nlp352', 
                            trim_ws = TRUE,
                            col_names = c('_gddid', 'sentence', 'wordIndex', 
                                          'word', 'partofspeech', 'specialclass', 
                                          'wordsAgain', 'wordtype', 'wordmodified'))

nlp_clean <- clean_corpus(x = full_nlp, pubs = publications) #uses the clean_corpus.R function

nlp<-nlp_clean$nlp

# assign the to_be_grouped file to group
group <- NULL
group <- nlp

#Aggregate the group for every 30 sentences in the groupNLP.R
group_30s = NULL
group_30s <- groupNLP(group,30)

# pass the result of grouping to nlp_group
nlp_group <- group_30s


```

To begin, we want to load the packages we will be using, and then import the data:
  
  
  
  From this we get an output object that includes a key for the publication (`_gddid`, linking to the `publications` variable), the sentence number of the parsed text, and then both the parsed text and some results from natural language processing. We also get a list of gddids to keep or drop given the regular expressions we used to find instances of IRD in the affiliations or references sections of the papers. This leaves us with 82 documents:
  
  ```{r warning=FALSE, echo = FALSE}

short_table <- nlp_group  %>% 
  filter(1:nrow(group_30s) %in% 1) %>%
  str_replace('-LRB-', '(') %>% 
  str_replace('-RRB-', ')') %>% 
  as.data.frame(stringsAsFactors = FALSE)

rownames(short_table) <- colnames(nlp_clean)
colnames(short_table) <- 'value'

short_table[nchar(short_table[,1])>40,1] <-
  paste0(substr(short_table[nchar(short_table[,1])>40, 1], 1, 30), ' . . .')

short_table %>% datatable()
```
From this we can compose two regular expressions.  Since we will be processing DMS coordinates differently than DD coordinates we generate two regular expressions.

## below are regular expressions to match coordinate pairs and age data:

```{r}
# Two types of coordinates:

dms_regex <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦o],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,2}(?:.[0-9]*)),[′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"
#dms_regex <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦oºø],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,3}(?:.[0-9]*)),[´′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"

dd_regex <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"
#dd_regex <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"

# ages and age ranges

========================================
{r, eval=FALSE}
<<<<<<< Updated upstream

is_date <- str_detect(full_nlp$word, "BP")

x$word[is_date&ird_word][4]

date_range <- str_detect(full_nlp$word,
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),")

date_match <- str_match(full_nlp$word,
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),") %>% na.omit()

browse(x = data.frame(gddid = x$`_gddid`[ird_word&!france],
words = x$word[ird_word&!france]),
pubs = publications)

number <- str_detect(full_nlp$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,")
========================================
##Output from pulling and cleaning dates using regex:

| Age ranges             |
| -----------            |
| "76,to,62,kyr,BP,"     |
| "6,--,6.4,kyr,BP,"     |
| "6,to,3,kyr,BP,"       |
| "11,--,10,kyr,BP,"     |
| "6.0,--,6.7,kyr,BP,"   |

| Age captures     |
| -----------      |
| "76"  "to" "62"  |
| "6"   "--" "6.4" |
| "6"   "to" "3"   |
| "11"  "--" "10"  |
| "6.0" "--" "6.7" |

| Date Label  |
| ----------- |
| "kyr,BP"    |
| "kyr,BP"    |
| "kyr,BP"    |
| "kyr,BP"    |
| "kyr,BP"    |

#extract "xx BP"
is_date <- str_detect(nlp_group$word, "BP")

# use this for xx--xx BP picker identifier
age_range_id <- str_detect(nlp_group$word,
                           "(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),")


age_match <- str_match(nlp_group$word, 
                       "(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),") %>% na.omit()


# use this for yr picker identifier
age_yr_id <- str_detect(nlp_group$word, 
                        ",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,")

age_yr2_id <- str_detect(nlp_group$word, 
                         ",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,")

# use this for ka picker identifier
age_ka_id <- str_detect(nlp_group$word, 
                        ",(\\d+(?:[\\.\\s]\\d+){0,1}),ka,")

# use this for BP picker identifier
age_bp_id <- str_detect(nlp_group$word, 
                        ",(\\d+(?:[\\.\\s]\\d+){0,1}),BP,")

age_range_regex <- "(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),"

age_yr_regex <-  ",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,"

age_yr2_regex <- ",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,"

age_ka_regex <- ",(\\d+(?:[\\.\\s]\\d+){0,1}),ka,"

age_bp_regex <- ",(\\d+(?:[\\.\\s]\\d+){0,1}),BP,"



```

These regular expressions allow for negative or positive coordinate systems, that may start with a `1`, and then are followed by one or two digits (`{1,2}`).  From there we see differences in the structure, reflecting the need to capture the degree symbols, or, in the case of decimal degrees, the decimal component of the coordinates.  We are more rigorous here for the decimal degrees because there are too many other options when there are only decimal numbers.

The regex commands were constructed using capture (and non-capture) groups to work with the `stringr` package, so that we obtain five elements from any match.  The full match, the degrees, the minutes and the seconds (which may be an empty string).  It also returns the quadrant (NESW).

```{r}
degmin <- str_match_all(nlp_group$word, dms_regex)
decdeg <- str_match_all(nlp_group$word, dd_regex)
```

Since the documents are broken up into sentences we should expect that all coordinates are reported as pairs, and so we might be most interested in finding all the records that show up with pairs of coordinates.  Let's start by matching up the publications with sentences that have coordinate pairs:

```{r, echo = FALSE}

coord_pairs <- sapply(degmin, function(x)length(x) %% 2 == 0 & length(x) > 0) |
  sapply(decdeg, function(x)length(x) %% 2 == 0 & length(x) > 0)


things <- nlp_group %>% 
  filter(coord_pairs) %>% 
  inner_join(publications, by = "_gddid") %>% 
  select(`_gddid`, word, year, title) %>% 
  mutate(word = gsub(',', ' ', word)) %>% 
  mutate(word = str_replace_all(word, '-LRB-', '(')) %>% 
  mutate(word = str_replace_all(word, '-RRB-', ')')) %>% 
  mutate(word = str_replace_all(word, '" "', ','))

things %>% select(-`_gddid`) %>% datatable

```
## Converting Coordinates

So, given the coordinate strings, we need to be able to transform them to reliable lat/long pairs with sufficient trust to actually map the records.  These two functions will convert the GeoDeepDive (GDD) word elements pulled out by the regular expression searches into decimal degrees that can account for reported locations.

```{r}

convert_dec <- function(x, i) {

drop_comma <- gsub(',', '', x) %>% 
substr(., c(1,1), nchar(.) - 1) %>% 
as.numeric %>% 
unlist

domain <- (str_detect(x, 'N') * 1 +
str_detect(x, 'E') * 1 +
str_detect(x, 'W') * -1 +
str_detect(x, 'S') * -1) *
drop_comma

publ <- match(nlp_group$`_gddid`[i], publications$`_gddid`)

check_date <- function(input1, input2, input3, input4, input5, index) {
if(isTRUE(input1[index])){
date_fun <- str_extract(nlp_group$word[index],
age_range_regex)
}
else if(isTRUE(input2[index])){
date_fun <- str_extract(nlp_group$word[index],
age_yr_regex)
}
else if(isTRUE(input3[index])){
date_fun <- str_extract(nlp_group$word[index],
age_yr2_regex)
}
else if(isTRUE(input4[index])){
date_fun <- str_extract(nlp_group$word[index],
age_ka_regex)
}
else if(isTRUE(input5[index])){
date_fun <- str_extract(nlp_group$word[index],
age_bp_regex)
}
else {date_fun <- "NA"}

return(date_fun)
}

point_pairs <- data.frame(sentence = nlp_group$word[i],
lat = domain[str_detect(x, 'N') | str_detect(x, 'S')],
lng = domain[str_detect(x, 'E') | str_detect(x, 'W')],
publications[publ,],
date = check_date(age_range_id, age_yr_id, age_yr2_id, age_ka_id,                                    age_bp_id, i),
stringsAsFactors = FALSE)

return(point_pairs)  
}

convert_dm <- function(x, i) {

# We use the `i` index so that we can keep the coordinate outputs from the 
#  regex in a smaller list.
dms <- data.frame(deg = as.numeric(x[,2]), 
min = as.numeric(x[,3]) / 60,
sec = as.numeric(x[,4]) / 60 ^ 2, 
stringsAsFactors = FALSE)

dms <- rowSums(dms, na.rm = TRUE)

domain <- (str_detect(x[,5], 'N') * 1 +
str_detect(x[,5], 'E') * 1 +
str_detect(x[,5], 'W') * -1 +
str_detect(x[,5], 'S') * -1) *
dms

publ <- match(nlp_group$`_gddid`[i], publications$`_gddid`)

check_date <- function(input1, input2, input3, input4, input5, index) {
if(isTRUE(input1[index])){
date_fun <- str_extract(nlp_group$word[index],
age_range_regex)
}
else if(isTRUE(input2[index])){
date_fun <- str_extract(nlp_group$word[index],
age_yr_regex)
}
else if(isTRUE(input3[index])){
date_fun <- str_extract(nlp_group$word[index],
age_yr2_regex)
}
else if(isTRUE(input4[index])){
date_fun <- str_extract(nlp_group$word[index],
age_ka_regex)
}
else if(isTRUE(input5[index])){
date_fun <- str_extract(nlp_group$word[index],
age_bp_regex)
}
else {date_fun <- "NA"}

return(date_fun)
}

point_pairs <- data.frame(sentence = nlp_group$word[i],
lat = domain[x[,5] %in% c('N', 'S')],
lng = domain[x[,5] %in% c('E', 'W')],
publications[publ,],
date = check_date(age_range_id, age_yr_id, age_yr2_id, age_ka_id,                                    age_bp_id, i),
stringsAsFactors = FALSE)

return(point_pairs)  
}

```

Then, once we've done that, we need to apply those functions to the set of records we've pulled to build a composite table:

```{r}
coordinates <- list()
coord_idx <- 1

for(i in 1:length(decdeg)) {

if((length(decdeg[[i]]) %% 2 == 0 | 
length(degmin[[i]]) %% 2 == 0) & length(degmin[[i]]) > 0) {

if(any(str_detect(decdeg[[i]], '[NS]')) & 
sum(str_detect(decdeg[[i]], '[EW]')) == sum(str_detect(decdeg[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dec(decdeg[[i]], i)
coord_idx <- coord_idx + 1
}
if(any(str_detect(degmin[[i]], '[NS]')) & 
sum(str_detect(degmin[[i]], '[EW]')) == sum(str_detect(degmin[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dm(degmin[[i]], i)
coord_idx <- coord_idx + 1
}
}
}

coordinates_df_group_30s <- coordinates %>% bind_rows %>% 
mutate(sentence = gsub(',', ' ', sentence)) %>% 
mutate(sentence = str_replace_all(sentence, '-LRB-', '(')) %>% 
                                                       mutate(sentence = str_replace_all(sentence, '-RRB-', ')')) %>% 
mutate(sentence = str_replace_all(sentence, '" "', ','))

coordinates_df_group_30s$doi <- coordinates_df_group_30s$identifier %>% map(function(x) x$id) %>% unlist

leaflet(coordinates_df_group_30s) %>% 
addProviderTiles(providers$CartoDB.Positron) %>% 
addCircleMarkers(popup = paste0('<b>', coordinates_df_group_30s$title, '</b><br>',
'<a href=https://doi.org/',
coordinates_df_group_30s$doi,'>Publication Link</a><br>',
'<b>Sentence:</b><br>',
'<small>',gsub(',', ' ', coordinates_df_group_30s$sentence),
'</small>'))

# generate an output csv file of the result
output <- coordinates_df_group_30s
output$author <- vapply(output$author, paste, collapse = ", ", character(1L))
output$link <- vapply(output$link, paste, collapse = ", ", character(1L))
output$identifier <- vapply(output$identifier, paste, collapse = ", ", character(1L))
write.csv(output, "30_stat.csv") 
```
